# -*- coding: utf-8 -*-
"""clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19UX8nLI52cUujlNR9FF5s_hqPMVA6xWh
"""

!pip install jupyterthemes
!pip install seaborn
!pip install missingno
!pip install vaderSentiment
!pip install pyLDAvis

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.colors
plt.style.use('ggplot')

import seaborn as sns
import missingno as msno

# from jupyterthemes import jtplot
# jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)

import re
import string
import json
import glob

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.stem import WordNetLemmatizer
nltk.download ('wordnet')

import spacy
!python -m spacy download en_core_web_md

#Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

#vis
import pyLDAvis
import pyLDAvis.gensim

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from wordcloud import WordCloud

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
from sklearn.decomposition import PCA

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

df = pd.read_csv('reviews_dataset.csv')
df.head()

print(df.shape)
df.info()

print(df.isnull().sum())
msno.matrix(df,figsize=(12,5), fontsize=12, color=(0.8, 0.3, 0.2))

df.ReviewTitle = df.ReviewTitle.apply(lambda x: re.sub(r"r/", "", x))
df.ReviewTitle = df.ReviewTitle.apply(lambda x: x.lower())
df.ReviewTitle.unique()

df['ReviewTitle'].value_counts().sort_values(ascending = False).plot(kind='bar', figsize=(50,5), width=0.2, color="#FF7B54")

plt.xlabel("ReviewTitle", fontsize=16, fontweight="semibold", labelpad=12)
plt.ylabel("Number of Comments", fontsize=14, fontweight="semibold", labelpad=12)
plt.title("Data Frequency for ReviewTitle", fontsize=18,pad=16, fontweight="bold")

plt.show()

# Adding two columns of number of words in each comment and the length of the comment.
df['word_count'] = df['ReviewTitle'].apply(lambda x : len(str(x).split(" ")))
df['length'] = df['ReviewTitle'].str.len()
df.describe()

# Droping the rows with word length smaller than 8

drop_index = df[df["word_count"] < 8 ].index
df.drop(drop_index, inplace= True)
df.shape

# Text Comment CLeaning
# defining some characters, stopwords that need to be removed

text_cleaning_re = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"
stop_words = stopwords.words('english')
stemmer = SnowballStemmer('english')
TAG_RE = re.compile(r'<[^>]+>')


# this function will just check wheather the argument is a number or not
def isDigit(x):
    try:
        float(x)
        return True
    except ValueError:
        return False

# A funtion to clean the text comments
def clean_text(sentence, stem=False):

    sentence = str(sentence)
    #Remove integers
    sentence = "".join([i for i in sentence if isDigit(i)==False])

    # Make all the text lower
    sentence = sentence.lower()

    # Remove html tags
    sentence = TAG_RE.sub('', sentence)

    # Remove speacial characters
    sentence = re.sub(text_cleaning_re, ' ',sentence)

    # Remove Punctuations
    sentence = sentence.translate(str.maketrans("", "", string.punctuation))

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)  # When we remove apostrophe from the word "Mark's", the apostrophe is replaced by an empty space. Hence, we are left with single character "s" that we are removing here.

    # Remove multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.

    # Remove stopwords and Stemming
    items = []
    for item in sentence.split():
        if item not in stop_words:
            if stem:
                items.append(stemmer.stem(item))
            else:
                items.append(item)

    sentence = " ".join(items)

    return sentence



# Replacing the text comment with cleaned one by passing them one by one to the clean text function
df.ReviewTitle = df.ReviewTitle.apply(lambda x: clean_text(x))
df.head()

# Generating TF-IDF Vector for KMeans from the comment_body
vectorizer = TfidfVectorizer(
                                lowercase=True,
                                ngram_range = (1,3),
                                stop_words = "english"

                            )

vectors = vectorizer.fit_transform(df.ReviewTitle)

k_range = range(1,10)
sse = []

for k in k_range:
    model = KMeans(n_clusters= k, init="k-means++", random_state=38, max_iter=100, n_init=1) #43, 45
    model.fit(vectors)
    sse.append(model.inertia_)

## Ploting SSE vs Number of Clusters and found the optimum number of clusters


plt.plot(k_range, sse, color="#FF7B54", marker = '.', markersize=14, linewidth = 3)
plt.xlabel("Number of Clusters", fontsize=16, fontweight="semibold", labelpad=12)
plt.ylabel("Sum of Squared Error", fontsize=14, fontweight="semibold", labelpad=12)
plt.title("Elbow plot", fontsize=18,pad=16, fontweight="bold")

# Labeling the rows of the dataframe with cluster labels
true_k = 4
model = KMeans(n_clusters=true_k, init="k-means++", random_state=48, max_iter=2000, n_init=1)
y_predicted = model.fit_predict(vectors)
df["cluster"] = y_predicted
df['cluster'].value_counts()

## Printing the Hotwords of every clusters

print("Cluster Hotwords")
hotwords = model.cluster_centers_.argsort()[:, ::-1]
feature_names = vectorizer.get_feature_names_out()

for i in range(true_k):
    print(f"Cluster {i}")
    for j in hotwords[i, :20]:
        print(f"{feature_names[j]}")
    print("-------------------------")

plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.cluster == 0 ].ReviewTitle))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.cluster == 1 ].ReviewTitle))
plt.imshow(wc , interpolation = 'bilinear')

df['cluster'].value_counts().sort_values(ascending = False).plot(kind='bar', figsize=(10,5), width=0.2, color="#FF7B54")

plt.xlabel("Comment Cluster", fontsize=16, fontweight="semibold", labelpad=12)
plt.ylabel("Number of Comments", fontsize=14, fontweight="semibold", labelpad=12)
plt.title("Clusterwise Data Frequency", fontsize=18,pad=16, fontweight="bold" )

plt.show()

# Creating list of Frequent words from comment text
freq = pd.Series(' '.join(df['ReviewTitle']).split()).value_counts()[:15]

# Removing Frequent words from comment text
df['comment_body'] = df['ReviewTitle'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))

# all_word_series
all_word_series = pd.Series(' '.join(df['ReviewTitle']).split()).value_counts()[-20000:]

# Creating list of words which has appeared less then 4 times in the entire corpus
rare = []
for i in range(0,792,1):
    if all_word_series.values[i] < 4:
        rare.append(all_word_series.index[i])

# Removing Rare words from comment text
df['ReviewTitle'] = df['ReviewTitle'].apply(lambda x: " ".join(x for x in x.split() if x not in rare))

# Creating nlp object
nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

def lemmatization(texts,allowed_postags=['NOUN', 'ADJ','VERB', 'ADV']):
       output = []
       for sent in texts:
             doc = nlp(sent)
             output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])
       return output

# lemmatization of the reviewTitle
text_list=df['ReviewTitle'].tolist()
print(text_list[1])
tokenized_reviews = lemmatization(text_list)
print(tokenized_reviews[1])

# Creating bigrams and tigrams for LDA

bigram_phrases = gensim.models.Phrases(tokenized_reviews, min_count=5, threshold=100)
trigram_phrases = gensim.models.Phrases(bigram_phrases[tokenized_reviews], threshold=100)

bigram = gensim.models.phrases.Phraser(bigram_phrases)
trigram = gensim.models.phrases.Phraser(trigram_phrases)

def make_bigrams(texts):
    return([bigram[doc] for doc in texts])

def make_trigrams(texts):
    return ([trigram[bigram[doc]] for doc in texts])

data_bigrams = make_bigrams(tokenized_reviews)
data_bigrams_trigrams = make_trigrams(data_bigrams)

# Removing frequent words from bigrams and tigrams and generating TF-IDF model
from gensim.models import TfidfModel

id2word = corpora.Dictionary(data_bigrams_trigrams)

texts = data_bigrams_trigrams

corpus = [id2word.doc2bow(text) for text in texts]

tfidf = TfidfModel(corpus, id2word=id2word)

low_value = 0.03
words  = []
words_missing_in_tfidf = []
for i in range(0, len(corpus)):
    bow = corpus[i]
    low_value_words = [] #reinitialize to be safe. You can skip this.
    tfidf_ids = [id for id, value in tfidf[bow]]
    bow_ids = [id for id, value in bow]
    low_value_words = [id for id, value in tfidf[bow] if value < low_value]
    drops = low_value_words+words_missing_in_tfidf
    for item in drops:
        words.append(id2word[item])
    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing

    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]
    corpus[i] = new_bow

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):

    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, random_state= 1)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

## Generating LDA model and calculating Coherence score for them.

model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=tokenized_reviews, start=1, limit=10, step=1)

# Plotting the Coherence socre for different number of topics to find out the optimum number of topics

limit=10; start=1; step=1;
x = range(start, limit, step)
plt.plot(x, coherence_values, color="#FF7B54", marker = '.', markersize=14, linewidth = 3)
plt.title("Optimum Number of Topics", fontsize=18, fontweight="bold", pad=16)
plt.xlabel("Number of Topics", fontsize=18, fontweight="semibold", labelpad=12)
plt.ylabel("Coherence Score", fontsize=16, fontweight="semibold", labelpad=12)
plt.show()

# Printing the coherence scores

for m, cv in zip(x, coherence_values):
    print("Num Topics =", m, " has Coherence Value of", round(cv, 4))

# Select the model and print the topics
optimal_model = model_list[4]
model_topics = optimal_model.show_topics(formatted=False)
optimal_model.print_topics(num_words=10)

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(optimal_model, corpus, id2word)
vis

# Obtaining the main topic for each review:

lda_corpus = optimal_model[corpus]

all_topics = optimal_model.get_document_topics(corpus)
all_topics
num_docs = len(all_topics)
num_docs
all_topics_csr = gensim.matutils.corpus2csc(all_topics)
all_topics_csr
all_topics_numpy= all_topics_csr.T.toarray()
all_topics_numpy
major_topic= [np.argmax(arr) for arr in all_topics_numpy]
df['major_topic']= major_topic
df.head()

# Plotting Data frequency for topics

df['major_topic'].value_counts().sort_values(ascending = False).plot(kind='bar',figsize=(10,5), width=0.2, color="#FF7B54")

plt.xlabel("Commet Topics", fontsize=16, fontweight="semibold", labelpad=12)
plt.ylabel("Number of Comments", fontsize=14, fontweight="semibold", labelpad=12)
plt.title("Topicwise Data Frequency", fontsize=18,pad=16, fontweight="bold" )

plt.show()

import plotly.express as px
import plotly.figure_factory as ff

import plotly.express as px
import plotly.figure_factory as ff

# color_map1 = matplotlib.colors.ListedColormap(["#EE6302", "#57B75A", "#BD323F", "#DDAC71", "#452547", "#D6E715"])
# ax = piv.plot(xticks=piv.index, ylabel="Number of comments", figsize=(15,10), fontsize=18, colormap=color_map1, marker = '.', markersize=20, linewidth = 3)

# ax.set_title("Topic Distribution in the CLusters", fontsize=22, pad=22, fontweight="bold")
# ax.set_xlabel("Comment Clusters", fontsize=20, fontweight="semibold", labelpad=20)
# ax.set_ylabel("Number of comments", fontsize=20, fontweight="semibold", labelpad=20)

# leg = plt.legend()
# for line in leg.get_lines():
#     line.set_linewidth(4)
# for text in leg.get_texts():
#     text.set_fontsize(20)

# plt.show()

## Creating Vader object for Sentiment Analysis
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm

analyser = SentimentIntensityAnalyzer()

# Calculating Polarity and adding the scores inside the existing dataframe
result = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    text = row['ReviewTitle']
    id = row['Author']
    result[id] = analyser.polarity_scores(text)

# Convert result dictionary into dataframe
df_sentiment = pd.DataFrame(result).T

# Merge result with the input dataframe
df_sentiment = df_sentiment.reset_index().rename(columns={'index':'Author'})
df_sentiment = df.merge(df_sentiment, how='left')
df_sentiment.head()

# Function to label the sentiment according to score
def get_tag(val):
    if val <= -0.50:
        return 'Negative'
    elif val > -0.5 and val < 0:
        return 'Somewhat Negative'
    elif val < 0.5 and val > 0:
        return 'Somewhat Positive'
    elif val >= 0.5:
        return 'Positive'
    else:
        return 'Neutral'

# Adding Sentiment Labels to dataframe

df_sentiment['Sentiment'] = df_sentiment['compound'].apply(get_tag)
df_sentiment.head()

fig, axs = plt.subplots(1, 3, figsize=(14, 4))

sns.barplot(data=df_sentiment, x='cluster', y='pos', ax=axs[0])
sns.barplot(data=df_sentiment, x='cluster', y='neu', ax=axs[1])
sns.barplot(data=df_sentiment, x='cluster', y='neg', ax=axs[2])

axs[0].set_title('Positivity', fontsize=16, pad=12, fontweight="bold" )
axs[1].set_title('Neutrality' ,fontsize=16, pad=12, fontweight="bold" )
axs[2].set_title('Negativity', fontsize=16, pad=12, fontweight="bold" )

axs[0].set_xlabel("Comment Clusters", fontsize=12, fontweight="semibold", labelpad=20)
axs[1].set_xlabel("Comment Clusters", fontsize=12, fontweight="semibold", labelpad=20)
axs[2].set_xlabel("Comment Clusters", fontsize=12, fontweight="semibold", labelpad=20)

axs[0].set_ylabel("Positive Score", fontsize=12, fontweight="semibold", labelpad=10)
axs[1].set_ylabel("Neutral Score", fontsize=12, fontweight="semibold", labelpad=10)
axs[2].set_ylabel("Negative Score", fontsize=12, fontweight="semibold", labelpad=10)

plt.tight_layout()
plt.show()

vis_dfS2 = pd.pivot_table(df_sentiment, index= "major_topic", columns = "Sentiment", aggfunc ="count")
vis_dfS2

ax = vis_dfS2.plot(xticks=vis_dfS2.index, ylabel="Number of comments", figsize=(15,10), fontsize=16)

ax.set_xlabel("Comment Topics", fontsize=20, fontweight="semibold", labelpad=20)
ax.set_ylabel("Number of comments", fontsize=20, fontweight="semibold", labelpad=20)
ax.set_title("Polarity in Comment Topics", fontsize=22,pad=22, fontweight="bold" )

ax.fill_between(vis_dfS2.index, vis_dfS2["Negative"], color = '#FF4A4A', alpha= 0.7)
ax.fill_between(vis_dfS2.index, vis_dfS2["Negative"], vis_dfS2["Somewhat Negative"], color = '#FF8787', alpha= 0.7)
ax.fill_between(vis_dfS2.index,vis_dfS2["Somewhat Negative"], vis_dfS2["Neutral"],color = '#FFD372' , alpha= 0.7)
ax.fill_between(vis_dfS2.index, vis_dfS2["Neutral"], vis_dfS2["Somewhat Positive"], color = '#C5D8A4', alpha= 0.7)
ax.fill_between(vis_dfS2.index,vis_dfS2["Somewhat Positive"], vis_dfS2["Positive"],color = '#829460' , alpha= 0.8)

plt.xlim(0,4)

leg = plt.legend(loc=(0.04, 0.77))
for line in leg.get_lines():
    line.set_linewidth(12)
for text in leg.get_texts():
    text.set_fontsize(18)

plt.show()