# -*- coding: utf-8 -*-
"""XGBClassifier-Using-ClassWeightedData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gX3isMv5eCl6ZMVoq4-yMGWphqgeVVso

*Assuming that the necessary libraries are imported

CLASS WEIGHTED DATA XGB CLASSIFIER
"""

from xgboost import XGBClassifier
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score
import numpy as np
import pandas as pd

# Assuming the data is preprocessed and loaded into the DataFrame df

# Shuffle the dataset
df = shuffle(df, random_state=42)

# Define features and target variable
X = df['ReviewContent']
y = df['Sentiment_encoded']

# Stratified split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# Compute class weights
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)
class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'scale_pos_weight': [class_weights_dict[i] for i in range(len(class_weights))],
    'objective': ['multi:softmax']
}

# Initialize XGBClassifier
xgb = XGBClassifier(random_state=42, use_label_encoder=False)

# Initialize RandomizedSearchCV
random_search_xgb = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid, scoring='accuracy',
                                       cv=5, n_iter=50, random_state=42, verbose=2, n_jobs=-1)

# Fit RandomizedSearchCV
random_search_xgb.fit(X_train_tfidf, y_train)

# Best parameters and estimator
best_params_xgb = random_search_xgb.best_params_
best_estimator_xgb = random_search_xgb.best_estimator_

print(f"Best Parameters (XGBoost): {best_params_xgb}")

# Predict on test data
y_pred_xgb = best_estimator_xgb.predict(X_test_tfidf)

# Evaluate model performance
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy (XGBoost): {accuracy_xgb:.2f}")

print("Classification Report (XGBoost):")
print(classification_report(y_test, y_pred_xgb))